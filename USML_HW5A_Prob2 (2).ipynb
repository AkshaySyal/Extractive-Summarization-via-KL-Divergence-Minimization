{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#PROBLEM 2: Extractive Summarization\n",
        "Implement the KL-Sum summarization method for each dataset. Follow the ideas in this paper ; you are allowed to use libraries for text cleaning, segmentation into sentences, etc. Run it twice :<br>\n",
        "A) KL_summary based on words_PD; PD is a distribution proportional to counts of words in document<br>\n",
        "B) LDA_summary based on LDA topics_PD on obtained in PB2. The only difference is that PD, while still a distribution over words, is computed using topic modeling<br>\n",
        "For DUC dataset evaluate KL_summaries and LDA_summaries against human gold summaries with ROUGE. ROUGE Perl package. Use the \"Abstract\" part of the files ins folder \"Summaries\" as the gold summaries.\n",
        "\n",
        "EXTRA CREDIT. KL Summarization: Can we make both PD and PS distributions over topics, instead of distributions over words? Would that help?"
      ],
      "metadata": {
        "id": "JDKJM7gFRjq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "2psXa2Hz9unt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.preprocessing import normalize\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "import scipy.sparse as sp\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import os\n",
        "import statistics\n",
        "from rouge import Rouge\n",
        "import math\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "tPqkWo5tUtg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6IsyhoYSRdVb"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/DUC/dataset.pkl'\n",
        "with open(path, 'rb') as file:\n",
        "    dataset = pickle.load(file)\n",
        "\n",
        "introductions = []\n",
        "file_name = []\n",
        "abstracts = []\n",
        "\n",
        "for data in dataset:\n",
        "  file_name.append(data)\n",
        "  introductions.append(dataset[data]['introduction'])\n",
        "  abstracts.append(dataset[data]['abstract'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = []\n",
        "unique_lengths = []\n",
        "for abstract in abstracts:\n",
        "  lengths.append(len(abstract.split()))\n",
        "  unique_lengths.append(len(set(abstract.split())))"
      ],
      "metadata": {
        "id": "hiZnM9TdtiUh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create summary of 100 words\n",
        "mean_value = statistics.mean(lengths)\n",
        "median_value = statistics.median(lengths)\n",
        "quartiles = np.percentile(lengths, [25, 50, 75,90])\n",
        "\n",
        "print(f\"Mean: {mean_value}\")\n",
        "print(f\"Median: {median_value}\")\n",
        "print(f\"25th percentile (Q1): {quartiles[0]}\")\n",
        "print(f\"50th percentile (Q2 or Median): {quartiles[1]}\")\n",
        "print(f\"75th percentile (Q3): {quartiles[2]}\")\n",
        "print(f\"95th percentile (Q3): {quartiles[3]}\")\n",
        "print(f\"Max: {max(lengths)}\")\n",
        "print(f\"Min: {min(lengths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blSsJMZltj_g",
        "outputId": "2ed7eefc-fb43-4018-d068-7ee400172a42"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 99.65016501650165\n",
            "Median: 101\n",
            "25th percentile (Q1): 98.0\n",
            "50th percentile (Q2 or Median): 101.0\n",
            "75th percentile (Q3): 103.0\n",
            "95th percentile (Q3): 106.0\n",
            "Max: 121\n",
            "Min: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_value = statistics.mean(unique_lengths)\n",
        "median_value = statistics.median(unique_lengths)\n",
        "quartiles = np.percentile(unique_lengths, [25, 50, 75,90])\n",
        "\n",
        "print(f\"Mean: {mean_value}\")\n",
        "print(f\"Median: {median_value}\")\n",
        "print(f\"25th percentile (Q1): {quartiles[0]}\")\n",
        "print(f\"50th percentile (Q2 or Median): {quartiles[1]}\")\n",
        "print(f\"75th percentile (Q3): {quartiles[2]}\")\n",
        "print(f\"95th percentile (Q3): {quartiles[3]}\")\n",
        "print(f\"Max: {max(unique_lengths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCyvmGZl7SoP",
        "outputId": "e0ecef14-ec7a-4e8d-94ff-d872e8686efa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 76.6963696369637\n",
            "Median: 78\n",
            "25th percentile (Q1): 73.0\n",
            "50th percentile (Q2 or Median): 78.0\n",
            "75th percentile (Q3): 82.0\n",
            "95th percentile (Q3): 85.0\n",
            "Max: 98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(lengths)):\n",
        "  if lengths[i] == 0:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djFu3CbcbLwM",
        "outputId": "c37ef9b8-59a5-4c5a-dc12-f5335d6025d6"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n",
            "194\n",
            "299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts.pop(45)\n",
        "abstracts.pop(194)\n",
        "abstracts.pop(229)\n",
        "introductions.pop(45)\n",
        "introductions.pop(194)\n",
        "introductions.pop(229)"
      ],
      "metadata": {
        "id": "r_wLcXK4bUG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  def is_valid_word(word):\n",
        "    return bool(wordnet.synsets(word))\n",
        "  text = re.sub(r'[^a-zA-Z0-9 \\n]', '', text) # removing any character that is not an alphanumeric character (letters and digits), a space, or a newline (\\n)\n",
        "  text = re.sub(r'\\n+', ' ', text) # removing new line\n",
        "  text = text.lower() # lower casing\n",
        "  tokens = word_tokenize(text)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [word for word in tokens if word not in stop_words] # removing stop words\n",
        "  filtered_tokens = [word for word in tokens if is_valid_word(word)] # removing non-english word\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens] # Lemmatization\n",
        "\n",
        "  return ' '.join(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "EalQB5Fu-OcJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kl_div(PD,PS):\n",
        "  sum_val = 0\n",
        "  for w in PS:\n",
        "    if w in PD:\n",
        "      sum_val += PD[w] * math.log(PD[w] / PS[w])\n",
        "  return sum_val"
      ],
      "metadata": {
        "id": "QVV-B9wm-5Ue"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_freq(doc):\n",
        "  word_counts = Counter(doc)\n",
        "  return {word: count for word, count in word_counts.items()}"
      ],
      "metadata": {
        "id": "Sg-Iare8A2w_"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_freq(doc):\n",
        "  lda= LatentDirichletAllocation(n_components=1, random_state=0, n_jobs=-1)\n",
        "  vectorizer = CountVectorizer()\n",
        "  doc_term_matrix = vectorizer.fit_transform([preprocess(doc)])\n",
        "  lda.fit(doc_term_matrix)\n",
        "  feature_names = vectorizer.get_feature_names_out()\n",
        "  top_n = 85\n",
        "  for idx, topic in enumerate(lda.components_):\n",
        "    topic_words = {feature_names[i]: 1 for i in np.argsort(topic)[::-1][:top_n]}\n",
        "  return topic_words"
      ],
      "metadata": {
        "id": "EXX0mhQ6FCA-"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###KL-Summary"
      ],
      "metadata": {
        "id": "TLcyzIgS-cUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KL_summary\n",
        "# Given a document -> get preprocessed doc (rmv stop words lematize etc) -> prep_doc\n",
        "# For each sentence s in prep_doc calculate KL_dist. Low KL_div => similar\n",
        "# Get s with lowest KL_dist, add to summary, break if until size of summary >= 100 -> list(idx(s))\n",
        "# Summary = document[list(idx(s))]"
      ],
      "metadata": {
        "id": "u3LiDLT4tzwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_kl_summary(doc):\n",
        "  prep_doc = preprocess(doc)\n",
        "  sent_list = doc.split('.')\n",
        "  processed_sent = [preprocess(sent) for sent in sent_list]\n",
        "  doc_pd = get_word_freq(prep_doc.split())\n",
        "\n",
        "  summary = ''\n",
        "  while(len(summary.split(' ')) <= 100 and len(processed_sent)>0):\n",
        "    min_kl_div = 10000\n",
        "    min_summary_sent_idx = 0\n",
        "\n",
        "    # finding which sentence when added to summary has lowest kl-div with doc\n",
        "    for idx, sent in enumerate(processed_sent):\n",
        "      summary_pd = get_word_freq(summary+sent)\n",
        "\n",
        "      if min_kl_div > kl_div(doc_pd,summary_pd):\n",
        "        min_kl_div = kl_div(doc_pd,summary_pd)\n",
        "        min_summary_sent_idx = idx\n",
        "\n",
        "    processed_sent.pop(min_summary_sent_idx)\n",
        "    summary += sent_list[min_summary_sent_idx]\n",
        "\n",
        "  return summary"
      ],
      "metadata": {
        "id": "zFIbEtsW-LKo"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(introductions[4],'\\n')\n",
        "print(get_kl_summary(introductions[4]),'\\n')\n",
        "print(abstracts[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_fII6dpSZNF7",
        "outputId": "589ab77a-f5de-4130-8e92-0762d6c00417"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQUADS of workers fanned out across storm-battered Louisiana yesterday to\n",
            "begin a massive rebuilding effort after Hurricane Andrew had flattened whole\n",
            "districts, killing two people and injuring dozens more, agencies report from\n",
            "Florida and New Orleans.\n",
            "However, local officials in Florida, hit earlier in the week by the\n",
            "hurricane, were critical of what they called a delay in supplying food,\n",
            "drinking water and other supplies for thousands of people in need.\n",
            "Federal emergency officials acknowledged distribution problems,\n",
            "Transportation Secretary Andrew Card yesterday promised 'dramatic'\n",
            "improvements within 24 hours and President George Bush last night ordered\n",
            "troops to Florida, without specifying a number.\n",
            "The government estimated it would cost Dollars 20bn-Dollars 30bn to tidy and\n",
            "rebuild in Florida, and to care for residents displaced by the storm.\n",
            "Louisiana state officials said they had no overall count of storm-related\n",
            "injuries but initial estimates reckoned fewer than 100. The Federal\n",
            "Emergency Management Agency said it was setting aside Dollars 77m to help\n",
            "Louisiana recover.\n",
            "Most of the storm's fury was spent against sparsely populated farming\n",
            "communities and swampland in the state, sparing it the widespread\n",
            "destruction caused in Florida, where 15 people died.\n",
            "Official estimates in Miami reported that the hurricane had wiped out the\n",
            "homes of one Dade County resident in eight - a quarter of a million people.\n",
            "Andrew had become little more than a strong rainstorm early yesterday,\n",
            "moving across Mississippi state and heading for the north-eastern US.\n",
            "Several of Louisiana's main industries were affected, including those of\n",
            "oysters and alligators. Wildlife and fisheries secretary Joe Herring\n",
            "estimated a 50 per cent decline in the alligator industry. The cotton and\n",
            "sugar-cane crops were threatened, the state agriculture department said.\n",
            "Most Louisiana oil refineries, however, were barely affected and deliveries\n",
            "of crude oil were expected to resume yesterday.\n",
            "\u001a \n",
            "\n",
            "SQUADS of workers fanned out across storm-battered Louisiana yesterday to\n",
            "begin a massive rebuilding effort after Hurricane Andrew had flattened whole\n",
            "districts, killing two people and injuring dozens more, agencies report from\n",
            "Florida and New Orleans The Federal\n",
            "Emergency Management Agency said it was setting aside Dollars 77m to help\n",
            "Louisiana recoverSQUADS of workers fanned out across storm-battered Louisiana yesterday to\n",
            "begin a massive rebuilding effort after Hurricane Andrew had flattened whole\n",
            "districts, killing two people and injuring dozens more, agencies report from\n",
            "Florida and New OrleansSQUADS of workers fanned out across storm-battered Louisiana yesterday to\n",
            "begin a massive rebuilding effort after Hurricane Andrew had flattened whole\n",
            "districts, killing two people and injuring dozens more, agencies report from\n",
            "Florida and New Orleans \n",
            "\n",
            "While workers fanned out across Louisiana yesterday for a massive rebuilding effort after the destruction caused by Hurricane Andrew, officials in Florida complained of delays in supplying critical assistance to thousands of people in need. Transportation Secretary Andrew Card promised \"dramatic\" improvements. In Dade County Florida it was reported that the hurricane had wiped out the homes of one resident in eight, affecting a quarter of a million people. The government estimated that it would cost $20-30 billion to rebuild and care for displaced residents in Florida. The Federal Emergency Management Agency said it was setting aside $77 million to help Louisiana recover.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "lDq59iVx-ekL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougeL_scores = []\n",
        "\n",
        "for idx, doc in enumerate(introductions):\n",
        "  summary = get_kl_summary(doc)\n",
        "  scores = rouge.get_scores(summary, abstracts[idx])\n",
        "  rouge1_scores.append(scores[0]['rouge-1']['f']) # f score\n",
        "  rouge2_scores.append(scores[0]['rouge-2']['f'])\n",
        "  rougeL_scores.append(scores[0]['rouge-l']['f'])"
      ],
      "metadata": {
        "id": "J1a8s3-06SP0"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Avg Rouge1: {statistics.mean(rouge1_scores)}\")\n",
        "print(f\"Avg Rouge2: {statistics.mean(rouge2_scores)}\")\n",
        "print(f\"Avg RougeL: {statistics.mean(rougeL_scores)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihe-H7-eaO2B",
        "outputId": "4fcda10f-eea0-4ba4-8a5e-9eb31521ac07"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Rouge1: 0.2873822469180531\n",
            "Avg Rouge2: 0.11254739557824144\n",
            "Avg RougeL: 0.24954993199899816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lHrOeO64cDAK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}